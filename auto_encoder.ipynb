{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = pd.read_pickle('nasdaq_prices.pkl')\n",
    "prices = prices['Adj Close']\n",
    "prices = prices.dropna(axis=1,how='any')\n",
    "prices = prices.reset_index()\n",
    "\n",
    "prices = prices.drop('Date', axis=1)\n",
    "\n",
    "prices = prices.pct_change().iloc[1:]\n",
    "\n",
    "prices_numpy = prices.values\n",
    "\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "prices_scaled = min_max_scaler.fit_transform(prices_numpy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.28507675, -0.26016192, -0.25467409, -0.23714014, -0.24962599,\n",
       "       -0.2671956 , -0.25054718, -0.24885651, -0.251042  , -0.2366159 ,\n",
       "       -0.24825266, -0.28026969, -0.24932058, -0.21631821, -0.2789594 ,\n",
       "       -0.28895852, -0.23217983, -0.25471199, -0.22947935, -0.27330096,\n",
       "       -0.24433931, -0.21866897, -0.23661669, -0.27047928, -0.25037167,\n",
       "       -0.24266509, -0.22805833, -0.23824428, -0.26051764, -0.27186306,\n",
       "       -0.26519288, -0.259196  , -0.24803261, -0.2682977 , -0.30874625,\n",
       "       -0.31296536, -0.27548207, -0.30308021, -0.30214877, -0.2588612 ,\n",
       "       -0.28776341, -0.22698108, -0.34136736, -0.25514086, -0.30291178,\n",
       "       -0.15107782, -0.28111592, -0.36686147, -0.20984003, -0.17762882,\n",
       "       -0.26641068, -0.41954061, -0.32966921, -0.24644575, -0.26055268,\n",
       "       -0.0147825 , -0.18220849, -0.24061113, -0.32085513, -0.33680191,\n",
       "       -0.25549492, -0.33385644, -0.29133219, -0.296417  , -0.2444772 ,\n",
       "       -0.20198162, -0.18021416, -0.18317358, -0.30261067, -0.22987156,\n",
       "       -0.23337671, -0.31808251, -0.2222909 , -0.28148073, -0.25584444,\n",
       "       -0.29625995, -0.25353808, -0.24836422, -0.27090147, -0.17017177,\n",
       "       -0.17155315, -0.28480345, -0.32790069, -0.30335911, -0.27318304,\n",
       "       -0.27037862, -0.23145375, -0.21053471, -0.25486273, -0.28179581,\n",
       "       -0.28935914, -0.24933445, -0.26021956, -0.1913538 , -0.26770136,\n",
       "       -0.23642288, -0.25090256, -0.2649878 , -0.15378952, -0.20223737,\n",
       "       -0.30760683, -0.28123955, -0.21371677, -0.24567995, -0.21500704,\n",
       "        0.02031598, -0.17807089, -0.19088379, -0.30971876, -0.30694843,\n",
       "       -0.35511915, -0.14340146, -0.2538312 , -0.23914101, -0.25419374,\n",
       "       -0.27138525, -0.27195396, -0.29701362, -0.2931419 , -0.29772422,\n",
       "       -0.24563466, -0.29202943, -0.20188918, -0.26469425, -0.26543975,\n",
       "       -0.26829626, -0.23632928, -0.2983608 , -0.24779164, -0.29705094,\n",
       "       -0.20716203, -0.26946566, -0.2556681 , -0.14505414, -0.30109951,\n",
       "       -0.28101252, -0.27674818, -0.25224649, -0.25860693, -0.22830992,\n",
       "       -0.27365877, -0.25224649, -0.23011985, -0.27309533, -0.26504537,\n",
       "       -0.25580582, -0.25463216, -0.22890219, -0.18922064, -0.22690065,\n",
       "       -0.2527551 , -0.20287423, -0.26503712, -0.26142427, -0.26400217,\n",
       "       -0.25075053, -0.28757162, -0.25329752, -0.24803559, -0.26166126,\n",
       "       -0.270286  , -0.1824342 , -0.2670504 , -0.2699121 , -0.22683788,\n",
       "       -0.23676475, -0.27859948, -0.24970542, -0.24566499, -0.24572961,\n",
       "       -0.239836  , -0.25127187, -0.28046848, -0.25427934, -0.25530516,\n",
       "       -0.23637024, -0.25824845, -0.21893424, -0.26041656, -0.2736563 ,\n",
       "       -0.30152263, -0.24518521, -0.27428205, -0.26114126, -0.22294552,\n",
       "       -0.22688342, -0.27875439, -0.25008089, -0.23659695, -0.23010411,\n",
       "       -0.24612449, -0.28207104, -0.22366412, -0.24767957, -0.25023064,\n",
       "       -0.26631471, -0.28817928, -0.24464829, -0.25922208, -0.23977387,\n",
       "       -0.2469237 , -0.23957357, -0.25483719, -0.23143971, -0.27998556,\n",
       "       -0.29435551, -0.28428327, -0.2687826 , -0.23952726, -0.24511513,\n",
       "       -0.25753812, -0.24869034, -0.2663951 , -0.21670625, -0.26025085,\n",
       "       -0.15154866, -0.29344631, -0.27046085, -0.26877184, -0.22400046,\n",
       "       -0.22244507, -0.25691339, -0.25015762, -0.24964359, -0.26572863,\n",
       "       -0.19772823, -0.19062018, -0.24687632, -0.25047081, -0.28987893,\n",
       "       -0.24567532, -0.22482551, -0.19735032, -0.23946856])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_scaler.inverse_transform(prices_numpy)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04950155, -0.0119349 , -0.00366034,  0.02277739,  0.00395119,\n",
       "       -0.0225403 ,  0.00256222,  0.00511142,  0.00181614,  0.02356784,\n",
       "        0.00602191, -0.04225345,  0.0044117 ,  0.05417275, -0.04027779,\n",
       "       -0.05535448,  0.03025657, -0.00371749,  0.03432836, -0.03174598,\n",
       "        0.01192247,  0.05062827,  0.02356666, -0.02749145,  0.00282685,\n",
       "        0.01444686,  0.03647097,  0.02111257, -0.01247126, -0.02957791,\n",
       "       -0.0195206 , -0.01047849,  0.0063537 , -0.02420205, -0.08519047,\n",
       "       -0.09155206, -0.03503466, -0.0766472 , -0.07524277, -0.00997368,\n",
       "       -0.0535525 ,  0.03809525, -0.13437667, -0.00436413, -0.07639325,\n",
       "        0.15254237, -0.0435294 , -0.17281675,  0.06394057,  0.11250871,\n",
       "       -0.02135679, -0.25224649, -0.11673817,  0.00874637, -0.0125241 ,\n",
       "        0.35804879,  0.10560347,  0.01754383, -0.10344827, -0.12749288,\n",
       "       -0.00489799, -0.12305169, -0.05893351, -0.0666004 ,  0.01171455,\n",
       "        0.0757895 ,  0.10861053,  0.10414831, -0.07593923,  0.03373697,\n",
       "        0.0284519 , -0.09926772,  0.04516712, -0.04407945, -0.00542499,\n",
       "       -0.06636359, -0.00194746,  0.0058537 , -0.02812803,  0.12375247,\n",
       "        0.12166961, -0.04908946, -0.1140716 , -0.07706773, -0.03156817,\n",
       "       -0.02733967,  0.03135135,  0.06289312, -0.00394477, -0.04455453,\n",
       "       -0.05595855,  0.00439078, -0.01202182,  0.09181415, -0.02330289,\n",
       "        0.02385887,  0.00202639, -0.01921138,  0.14845367,  0.07540388,\n",
       "       -0.08347246, -0.04371581,  0.05809521,  0.00990105,  0.05614974,\n",
       "        0.41097036,  0.11184216,  0.09252282, -0.08665683, -0.08247972,\n",
       "       -0.15511165,  0.1641168 , -0.00238943,  0.01976047, -0.00293606,\n",
       "       -0.02885747, -0.02971497, -0.0675    , -0.0616622 , -0.06857143,\n",
       "        0.00996933, -0.05998481,  0.07592888, -0.01876877, -0.01989283,\n",
       "       -0.02419988,  0.02400002, -0.06953128,  0.00671704, -0.06755625,\n",
       "        0.06797847, -0.02596311, -0.00515911,  0.16162489, -0.0736607 ,\n",
       "       -0.04337349, -0.03694371,  0.        , -0.00959029,  0.03609162,\n",
       "       -0.03228548,  0.        ,  0.03336261, -0.03143592, -0.01929819,\n",
       "       -0.00536676, -0.00359712,  0.03519859,  0.09503053,  0.03821652,\n",
       "       -0.00076689,  0.07444361, -0.01928575, -0.01383828, -0.01772524,\n",
       "        0.00225562, -0.05326332, -0.00158475,  0.0063492 , -0.01419561,\n",
       "       -0.02720001,  0.10526314, -0.02232137, -0.02663625,  0.03831116,\n",
       "        0.02334341, -0.0397351 ,  0.00383143,  0.0099236 ,  0.00982616,\n",
       "        0.01871258,  0.00146954, -0.04255319, -0.00306513, -0.00461187,\n",
       "        0.02393826, -0.00904976,  0.0502283 , -0.01231885, -0.03228177,\n",
       "       -0.07429868,  0.01064702, -0.03322527, -0.01341155,  0.04418008,\n",
       "        0.0382425 , -0.03996867,  0.0032653 ,  0.02359642,  0.03338633,\n",
       "        0.00923076, -0.04496952,  0.04309657,  0.00688601,  0.00303951,\n",
       "       -0.0212121 , -0.05417955,  0.01145658, -0.01051781,  0.01880626,\n",
       "        0.00802571,  0.01910826, -0.00390626,  0.03137252, -0.04182504,\n",
       "       -0.06349208, -0.04830514, -0.02493319,  0.01917809,  0.01075268,\n",
       "       -0.00797874,  0.00536197, -0.02133331,  0.05358766, -0.01206899,\n",
       "        0.15183244, -0.06212119, -0.02746366, -0.02491696,  0.04258944,\n",
       "        0.04493466, -0.00703676,  0.0031496 ,  0.00392466, -0.0203284 ,\n",
       "        0.08220277,  0.0929203 ,  0.00809716,  0.00267737, -0.05674229,\n",
       "        0.00990802,  0.04134542,  0.08277258,  0.01926659])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_numpy[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    " \n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 128\n",
    "# image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fde771441404e2ebe600888fa5b8ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d065a6e7452241f19b1e01c9c2dbe6a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b679f7e5b81b4e43a10cf413c3d40169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e39b01d5a334741b7289ff2977cd2a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulrice/opt/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1607370249289/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "trainset = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=True, \n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "testset = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "trainloader = DataLoader(\n",
    "    trainset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "testloader = DataLoader(\n",
    "    testset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda:0'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return device\n",
    "def make_dir():\n",
    "    image_dir = 'FashionMNIST_Images'\n",
    "    if not os.path.exists(image_dir):\n",
    "        os.makedirs(image_dir)\n",
    "def save_decoded_image(img, epoch):\n",
    "    img = img.view(img.size(0), 1, 28, 28)\n",
    "    save_image(img, './FashionMNIST_Images/linear_ae_image{}.png'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, feature_len):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # encoder\n",
    "        self.enc1 = nn.Linear(in_features=feature_len, out_features=256)\n",
    "        self.enc2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.enc3 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.enc4 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.enc5 = nn.Linear(in_features=32, out_features=16)\n",
    "        # decoder \n",
    "        self.dec1 = nn.Linear(in_features=16, out_features=32)\n",
    "        self.dec2 = nn.Linear(in_features=32, out_features=64)\n",
    "        self.dec3 = nn.Linear(in_features=64, out_features=128)\n",
    "        self.dec4 = nn.Linear(in_features=128, out_features=256)\n",
    "        self.dec5 = nn.Linear(in_features=256, out_features=feature_len)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.enc1(x))\n",
    "        x = F.relu(self.enc2(x))\n",
    "        x = F.relu(self.enc3(x))\n",
    "        x = F.relu(self.enc4(x))\n",
    "        x = F.relu(self.enc5(x))\n",
    "        x = F.relu(self.dec1(x))\n",
    "        x = F.relu(self.dec2(x))\n",
    "        x = F.relu(self.dec3(x))\n",
    "        x = F.relu(self.dec4(x))\n",
    "        x = F.relu(self.dec5(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (enc1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (enc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (enc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (enc4): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (enc5): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (dec1): Linear(in_features=16, out_features=32, bias=True)\n",
      "  (dec2): Linear(in_features=32, out_features=64, bias=True)\n",
      "  (dec3): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (dec4): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (dec5): Linear(in_features=256, out_features=784, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Autoencoder(feature_len=784)\n",
    "print(net)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainloader, NUM_EPOCHS):\n",
    "    train_loss = []\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for data in trainloader:\n",
    "            img, _ = data\n",
    "            img = img.to(device)\n",
    "            img = img.view(img.size(0), -1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(img)\n",
    "            loss = criterion(outputs, img)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        loss = running_loss / len(trainloader)\n",
    "        train_loss.append(loss)\n",
    "        print('Epoch {} of {}, Train Loss: {:.3f}'.format(\n",
    "            epoch+1, NUM_EPOCHS, loss))\n",
    "        if epoch % 5 == 0:\n",
    "            save_decoded_image(outputs.cpu().data, epoch)\n",
    "    return train_loss\n",
    "def test_image_reconstruction(net, testloader):\n",
    "     for batch in testloader:\n",
    "        img, _ = batch\n",
    "        img = img.to(device)\n",
    "        img = img.view(img.size(0), -1)\n",
    "        outputs = net(img)\n",
    "        outputs = outputs.view(outputs.size(0), 1, 28, 28).cpu().data\n",
    "        save_image(outputs, 'fashionmnist_reconstruction.png')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the computation device\n",
    "device = get_device()\n",
    "print(device)\n",
    "# load the neural network onto the device\n",
    "net.to(device)\n",
    "make_dir()\n",
    "# train the network\n",
    "train_loss = train(net, trainloader, NUM_EPOCHS)\n",
    "plt.figure()\n",
    "plt.plot(train_loss)\n",
    "plt.title('Train Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig('deep_ae_fashionmnist_loss.png')\n",
    "# test the network\n",
    "test_image_reconstruction(net, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainloader.dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader.dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1922, 0.0627, 0.0000,\n",
      "        0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0000, 0.0667, 0.0510, 0.0000,\n",
      "        0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0275, 0.0000, 0.5961, 0.4314,\n",
      "        0.0000, 0.0275, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0353, 0.0000, 0.5412, 0.4980,\n",
      "        0.0000, 0.0235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.6157,\n",
      "        0.6353, 0.0000, 0.0314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0275, 0.0000, 0.6824,\n",
      "        0.5294, 0.0000, 0.0314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000,\n",
      "        0.5569, 0.7529, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0000, 0.0000,\n",
      "        0.8314, 0.4667, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0235,\n",
      "        0.0000, 0.4941, 0.7804, 0.0627, 0.0000, 0.0196, 0.0118, 0.0078, 0.0039,\n",
      "        0.0039, 0.0039, 0.0039, 0.0000, 0.0000, 0.0039, 0.0039, 0.0157, 0.0000,\n",
      "        0.0000, 0.8667, 0.4235, 0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0275, 0.0000, 0.5176, 0.6745, 0.3020, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.1686, 0.8275, 0.3294, 0.0000, 0.0196, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0353, 0.0000, 0.4706, 0.4745, 0.5255, 0.0000, 0.0275, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.3569, 0.7020, 0.3647, 0.0000, 0.0157, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0314, 0.0000, 0.3804, 0.6627, 0.8392, 1.0000, 1.0000,\n",
      "        0.9961, 0.9608, 0.9255, 0.9020, 0.8863, 0.8706, 0.8510, 0.8510, 0.8706,\n",
      "        0.8510, 0.8627, 0.8314, 0.7529, 0.7765, 0.2863, 0.0000, 0.0118, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0314, 0.0000, 0.2863, 0.9294, 0.8196, 0.8118,\n",
      "        0.8235, 0.8275, 0.8392, 0.8392, 0.8471, 0.8627, 0.8706, 0.8745, 0.8627,\n",
      "        0.8627, 0.8588, 0.8510, 0.8627, 0.8353, 0.8745, 0.2314, 0.0000, 0.0118,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0000, 0.2980, 0.9255, 0.8235,\n",
      "        0.8392, 0.8196, 0.8118, 0.8078, 0.8157, 0.8196, 0.8235, 0.8196, 0.8118,\n",
      "        0.7922, 0.8078, 0.8275, 0.8392, 0.8275, 0.8353, 0.9098, 0.2353, 0.0000,\n",
      "        0.0157, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6667, 0.9020,\n",
      "        0.8196, 0.8431, 0.8431, 0.8510, 0.8549, 0.8392, 0.8275, 0.8275, 0.8431,\n",
      "        0.8431, 0.8353, 0.8471, 0.8392, 0.8471, 0.8588, 0.8510, 0.9412, 0.2667,\n",
      "        0.0000, 0.0157, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6314,\n",
      "        0.8824, 0.8157, 0.8314, 0.8275, 0.8353, 0.8431, 0.8275, 0.8314, 0.8431,\n",
      "        0.8431, 0.8353, 0.8314, 0.8275, 0.8235, 0.8392, 0.8549, 0.8510, 0.9059,\n",
      "        0.2000, 0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.5020, 0.8706, 0.7922, 0.8549, 0.8275, 0.8353, 0.8392, 0.8314, 0.8353,\n",
      "        0.8431, 0.8392, 0.8431, 0.8431, 0.8431, 0.8431, 0.8431, 0.8431, 0.8510,\n",
      "        0.8863, 0.1451, 0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.7176, 0.8941, 0.8431, 0.8667, 0.8392, 0.8549, 0.8588, 0.8549,\n",
      "        0.8627, 0.8667, 0.8667, 0.8627, 0.8706, 0.8824, 0.8706, 0.8784, 0.8745,\n",
      "        0.8667, 0.9137, 0.1765, 0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0039, 0.0000, 0.5216, 0.9333, 0.8745, 0.8824, 0.8627, 0.8588, 0.8784,\n",
      "        0.8784, 0.8745, 0.8706, 0.8745, 0.8745, 0.8745, 0.8745, 0.8745, 0.8863,\n",
      "        0.8980, 0.8745, 0.8941, 0.1765, 0.0000, 0.0118, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0118, 0.0000, 0.2392, 0.9333, 0.8588, 0.8980, 0.8627, 0.8549,\n",
      "        0.8627, 0.8667, 0.8667, 0.8627, 0.8588, 0.8588, 0.8510, 0.8471, 0.8510,\n",
      "        0.8745, 0.8980, 0.8588, 0.8588, 0.1294, 0.0000, 0.0235, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0275, 0.0000, 0.1569, 0.9255, 0.8627, 0.9059, 0.8980,\n",
      "        0.8627, 0.8471, 0.8510, 0.8549, 0.8510, 0.8471, 0.8431, 0.8471, 0.8588,\n",
      "        0.8627, 0.8784, 0.9059, 0.8627, 0.8706, 0.0980, 0.0000, 0.0196, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0275, 0.0000, 0.1686, 0.9098, 0.8627, 0.9137,\n",
      "        0.8980, 0.8706, 0.8588, 0.8471, 0.8353, 0.8471, 0.8588, 0.8627, 0.8667,\n",
      "        0.8824, 0.8863, 0.8824, 0.8980, 0.8745, 0.8824, 0.1137, 0.0000, 0.0157,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0157, 0.0000, 0.1686, 1.0000, 0.8745,\n",
      "        0.9176, 0.9098, 0.8863, 0.8706, 0.8588, 0.8510, 0.8745, 0.8902, 0.8902,\n",
      "        0.8706, 0.8902, 0.9059, 0.8706, 0.8745, 0.8784, 1.0000, 0.0863, 0.0000,\n",
      "        0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0000, 0.1490, 1.0000,\n",
      "        0.8667, 0.9059, 0.8863, 0.8902, 0.8941, 0.8667, 0.8627, 0.8784, 0.8863,\n",
      "        0.8706, 0.8784, 0.8941, 0.9059, 0.8824, 0.8588, 0.8863, 0.9686, 0.0471,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4275,\n",
      "        1.0000, 0.8157, 0.8353, 0.8510, 0.9020, 0.9020, 0.8510, 0.8588, 0.8588,\n",
      "        0.8431, 0.8353, 0.9059, 0.9098, 0.9020, 0.9020, 0.8784, 0.8275, 1.0000,\n",
      "        0.0235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1373,\n",
      "        1.0000, 0.9255, 0.8235, 0.8980, 0.9294, 0.9137, 0.8824, 0.8510, 0.8667,\n",
      "        0.8549, 0.8431, 0.8667, 0.8863, 0.9333, 0.9137, 0.8471, 0.8667, 0.8471,\n",
      "        0.9294, 0.8157, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1686,\n",
      "        1.0000, 0.9451, 0.9059, 0.8706, 1.0000, 0.9490, 0.8824, 0.8588, 0.8510,\n",
      "        0.8706, 0.8431, 0.8275, 0.8941, 0.8667, 0.9373, 0.9569, 0.8627, 0.8667,\n",
      "        0.8667, 0.8392, 0.9843, 0.3961, 0.0000, 0.0000, 0.0000, 0.0000, 0.1451,\n",
      "        0.8902, 0.9451, 0.9412, 0.8824, 0.8588, 0.9725, 0.9098, 0.8745, 0.8706,\n",
      "        0.8627, 0.8667, 0.8588, 0.8314, 0.9020, 0.8510, 0.9255, 0.9922, 0.9176,\n",
      "        0.8863, 0.8824, 0.8667, 0.8784, 0.8353, 0.0000, 0.0000, 0.0000, 0.1294,\n",
      "        0.9216, 1.0000, 0.9333, 0.8902, 0.8863, 1.0000, 1.0000, 0.9608, 0.8667,\n",
      "        0.8471, 0.8824, 0.8941, 0.8314, 0.9333, 0.9373, 0.8588, 0.9098, 0.9529,\n",
      "        0.8980, 0.8392, 0.8745, 0.8706, 0.8549, 0.9333, 0.7647, 0.0000, 0.0000,\n",
      "        0.0000, 0.3020, 0.6784, 0.9059, 0.9333, 0.8902, 0.4784, 0.6196, 0.9098,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 0.1882, 0.2431, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9961, 0.8510, 0.1333, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0431, 0.7373, 0.3882, 0.0000, 0.0000,\n",
      "        0.0000, 0.0863, 0.1216, 0.1098, 0.0000, 0.0000, 0.0000, 0.1059, 0.3843,\n",
      "        0.2431, 0.2902, 0.3804, 0.4431, 0.2549, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000])\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for data in trainloader:\n",
    "    img, _ = data\n",
    "    img = img.to(device)\n",
    "    img = img.view(img.size(0), -1)\n",
    "    print(img.data[0])\n",
    "    break\n",
    "    total+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60032"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total * 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (enc1): Linear(in_features=2786, out_features=256, bias=True)\n",
      "  (enc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (enc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (enc4): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (enc5): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (dec1): Linear(in_features=16, out_features=32, bias=True)\n",
      "  (dec2): Linear(in_features=32, out_features=64, bias=True)\n",
      "  (dec3): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (dec4): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (dec5): Linear(in_features=256, out_features=2786, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Autoencoder(feature_len=prices.shape[1])\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (enc1): Linear(in_features=2786, out_features=256, bias=True)\n",
      "  (enc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (enc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (enc4): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (enc5): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (dec1): Linear(in_features=16, out_features=32, bias=True)\n",
      "  (dec2): Linear(in_features=32, out_features=64, bias=True)\n",
      "  (dec3): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (dec4): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (dec5): Linear(in_features=256, out_features=2786, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Autoencoder(feature_len=prices_numpy.shape[1])\n",
    "print(net)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (enc1): Linear(in_features=2786, out_features=256, bias=True)\n",
      "  (enc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (enc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (enc4): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (enc5): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (dec1): Linear(in_features=16, out_features=32, bias=True)\n",
      "  (dec2): Linear(in_features=32, out_features=64, bias=True)\n",
      "  (dec3): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (dec4): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (dec5): Linear(in_features=256, out_features=2786, bias=True)\n",
      ")\n",
      "Epoch 1 of 50, Train Loss: 0.0004598195\n",
      "Epoch 2 of 50, Train Loss: 0.0004510943\n",
      "Epoch 3 of 50, Train Loss: 0.0004409455\n",
      "Epoch 4 of 50, Train Loss: 0.0004290288\n",
      "Epoch 5 of 50, Train Loss: 0.0004150474\n",
      "Epoch 6 of 50, Train Loss: 0.0003983775\n",
      "Epoch 7 of 50, Train Loss: 0.0003789245\n",
      "Epoch 8 of 50, Train Loss: 0.0003560928\n",
      "Epoch 9 of 50, Train Loss: 0.0003296116\n",
      "Epoch 10 of 50, Train Loss: 0.0002998040\n",
      "Epoch 11 of 50, Train Loss: 0.0002691037\n",
      "Epoch 12 of 50, Train Loss: 0.0002458454\n",
      "Epoch 13 of 50, Train Loss: 0.0002499698\n",
      "Epoch 14 of 50, Train Loss: 0.0002576786\n",
      "Epoch 15 of 50, Train Loss: 0.0002432281\n",
      "Epoch 16 of 50, Train Loss: 0.0002243187\n",
      "Epoch 17 of 50, Train Loss: 0.0002118554\n",
      "Epoch 18 of 50, Train Loss: 0.0002063151\n",
      "Epoch 19 of 50, Train Loss: 0.0002046369\n",
      "Epoch 20 of 50, Train Loss: 0.0002041134\n",
      "Epoch 21 of 50, Train Loss: 0.0002034082\n",
      "Epoch 22 of 50, Train Loss: 0.0002022018\n",
      "Epoch 23 of 50, Train Loss: 0.0002006081\n",
      "Epoch 24 of 50, Train Loss: 0.0001989241\n",
      "Epoch 25 of 50, Train Loss: 0.0001975110\n",
      "Epoch 26 of 50, Train Loss: 0.0001966432\n",
      "Epoch 27 of 50, Train Loss: 0.0001963529\n",
      "Epoch 28 of 50, Train Loss: 0.0001964007\n",
      "Epoch 29 of 50, Train Loss: 0.0001963372\n",
      "Epoch 30 of 50, Train Loss: 0.0001957880\n",
      "Epoch 31 of 50, Train Loss: 0.0001946387\n",
      "Epoch 32 of 50, Train Loss: 0.0001930992\n",
      "Epoch 33 of 50, Train Loss: 0.0001915254\n",
      "Epoch 34 of 50, Train Loss: 0.0001902234\n",
      "Epoch 35 of 50, Train Loss: 0.0001893234\n",
      "Epoch 36 of 50, Train Loss: 0.0001887915\n",
      "Epoch 37 of 50, Train Loss: 0.0001884897\n",
      "Epoch 38 of 50, Train Loss: 0.0001882717\n",
      "Epoch 39 of 50, Train Loss: 0.0001880423\n",
      "Epoch 40 of 50, Train Loss: 0.0001877639\n",
      "Epoch 41 of 50, Train Loss: 0.0001874639\n",
      "Epoch 42 of 50, Train Loss: 0.0001871771\n",
      "Epoch 43 of 50, Train Loss: 0.0001869668\n",
      "Epoch 44 of 50, Train Loss: 0.0001868579\n",
      "Epoch 45 of 50, Train Loss: 0.0001868265\n",
      "Epoch 46 of 50, Train Loss: 0.0001868194\n",
      "Epoch 47 of 50, Train Loss: 0.0001867857\n",
      "Epoch 48 of 50, Train Loss: 0.0001866852\n",
      "Epoch 49 of 50, Train Loss: 0.0001865388\n",
      "Epoch 50 of 50, Train Loss: 0.0001863797\n",
      "Epoch 51 of 50, Train Loss: 0.0001862428\n",
      "Epoch 52 of 50, Train Loss: 0.0001861434\n",
      "Epoch 53 of 50, Train Loss: 0.0001860722\n",
      "Epoch 54 of 50, Train Loss: 0.0001860092\n",
      "Epoch 55 of 50, Train Loss: 0.0001859385\n",
      "Epoch 56 of 50, Train Loss: 0.0001858649\n",
      "Epoch 57 of 50, Train Loss: 0.0001857772\n",
      "Epoch 58 of 50, Train Loss: 0.0001857124\n",
      "Epoch 59 of 50, Train Loss: 0.0001856826\n",
      "Epoch 60 of 50, Train Loss: 0.0001856663\n",
      "Epoch 61 of 50, Train Loss: 0.0001856272\n",
      "Epoch 62 of 50, Train Loss: 0.0001855697\n",
      "Epoch 63 of 50, Train Loss: 0.0001854895\n",
      "Epoch 64 of 50, Train Loss: 0.0001854322\n",
      "Epoch 65 of 50, Train Loss: 0.0001853804\n",
      "Epoch 66 of 50, Train Loss: 0.0001853154\n",
      "Epoch 67 of 50, Train Loss: 0.0001852344\n",
      "Epoch 68 of 50, Train Loss: 0.0001851496\n",
      "Epoch 69 of 50, Train Loss: 0.0001850788\n",
      "Epoch 70 of 50, Train Loss: 0.0001850221\n",
      "Epoch 71 of 50, Train Loss: 0.0001849628\n",
      "Epoch 72 of 50, Train Loss: 0.0001848920\n",
      "Epoch 73 of 50, Train Loss: 0.0001848236\n",
      "Epoch 74 of 50, Train Loss: 0.0001847701\n",
      "Epoch 75 of 50, Train Loss: 0.0001847246\n",
      "Epoch 76 of 50, Train Loss: 0.0001846740\n",
      "Epoch 77 of 50, Train Loss: 0.0001846196\n",
      "Epoch 78 of 50, Train Loss: 0.0001845728\n",
      "Epoch 79 of 50, Train Loss: 0.0001845338\n",
      "Epoch 80 of 50, Train Loss: 0.0001844910\n",
      "Epoch 81 of 50, Train Loss: 0.0001844450\n",
      "Epoch 82 of 50, Train Loss: 0.0001844015\n",
      "Epoch 83 of 50, Train Loss: 0.0001843606\n",
      "Epoch 84 of 50, Train Loss: 0.0001843137\n",
      "Epoch 85 of 50, Train Loss: 0.0001842634\n",
      "Epoch 86 of 50, Train Loss: 0.0001842177\n",
      "Epoch 87 of 50, Train Loss: 0.0001841667\n",
      "Epoch 88 of 50, Train Loss: 0.0001841181\n",
      "Epoch 89 of 50, Train Loss: 0.0001840631\n",
      "Epoch 90 of 50, Train Loss: 0.0001840060\n",
      "Epoch 91 of 50, Train Loss: 0.0001839336\n",
      "Epoch 92 of 50, Train Loss: 0.0001838572\n",
      "Epoch 93 of 50, Train Loss: 0.0001837777\n",
      "Epoch 94 of 50, Train Loss: 0.0001837054\n",
      "Epoch 95 of 50, Train Loss: 0.0001836296\n",
      "Epoch 96 of 50, Train Loss: 0.0001835526\n",
      "Epoch 97 of 50, Train Loss: 0.0001834801\n",
      "Epoch 98 of 50, Train Loss: 0.0001833767\n",
      "Epoch 99 of 50, Train Loss: 0.0001832683\n",
      "Epoch 100 of 50, Train Loss: 0.0001831502\n",
      "Epoch 101 of 50, Train Loss: 0.0001830381\n",
      "Epoch 102 of 50, Train Loss: 0.0001829297\n",
      "Epoch 103 of 50, Train Loss: 0.0001828103\n",
      "Epoch 104 of 50, Train Loss: 0.0001826985\n",
      "Epoch 105 of 50, Train Loss: 0.0001825854\n",
      "Epoch 106 of 50, Train Loss: 0.0001824712\n",
      "Epoch 107 of 50, Train Loss: 0.0001823606\n",
      "Epoch 108 of 50, Train Loss: 0.0001822410\n",
      "Epoch 109 of 50, Train Loss: 0.0001821064\n",
      "Epoch 110 of 50, Train Loss: 0.0001819910\n",
      "Epoch 111 of 50, Train Loss: 0.0001818878\n",
      "Epoch 112 of 50, Train Loss: 0.0001817736\n",
      "Epoch 113 of 50, Train Loss: 0.0001816831\n",
      "Epoch 114 of 50, Train Loss: 0.0001816225\n",
      "Epoch 115 of 50, Train Loss: 0.0001815612\n",
      "Epoch 116 of 50, Train Loss: 0.0001814926\n",
      "Epoch 117 of 50, Train Loss: 0.0001814333\n",
      "Epoch 118 of 50, Train Loss: 0.0001813786\n",
      "Epoch 119 of 50, Train Loss: 0.0001813060\n",
      "Epoch 120 of 50, Train Loss: 0.0001812014\n",
      "Epoch 121 of 50, Train Loss: 0.0001810486\n",
      "Epoch 122 of 50, Train Loss: 0.0001808865\n",
      "Epoch 123 of 50, Train Loss: 0.0001807327\n",
      "Epoch 124 of 50, Train Loss: 0.0001805662\n",
      "Epoch 125 of 50, Train Loss: 0.0001804050\n",
      "Epoch 126 of 50, Train Loss: 0.0001802656\n",
      "Epoch 127 of 50, Train Loss: 0.0001801193\n",
      "Epoch 128 of 50, Train Loss: 0.0001799515\n",
      "Epoch 129 of 50, Train Loss: 0.0001797999\n",
      "Epoch 130 of 50, Train Loss: 0.0001797086\n",
      "Epoch 131 of 50, Train Loss: 0.0001796445\n",
      "Epoch 132 of 50, Train Loss: 0.0001795434\n",
      "Epoch 133 of 50, Train Loss: 0.0001794078\n",
      "Epoch 134 of 50, Train Loss: 0.0001792911\n",
      "Epoch 135 of 50, Train Loss: 0.0001791930\n",
      "Epoch 136 of 50, Train Loss: 0.0001790885\n",
      "Epoch 137 of 50, Train Loss: 0.0001789469\n",
      "Epoch 138 of 50, Train Loss: 0.0001788046\n",
      "Epoch 139 of 50, Train Loss: 0.0001786869\n",
      "Epoch 140 of 50, Train Loss: 0.0001785510\n",
      "Epoch 141 of 50, Train Loss: 0.0001784140\n",
      "Epoch 142 of 50, Train Loss: 0.0001782803\n",
      "Epoch 143 of 50, Train Loss: 0.0001781177\n",
      "Epoch 144 of 50, Train Loss: 0.0001779630\n",
      "Epoch 145 of 50, Train Loss: 0.0001777507\n",
      "Epoch 146 of 50, Train Loss: 0.0001775146\n",
      "Epoch 147 of 50, Train Loss: 0.0001772784\n",
      "Epoch 148 of 50, Train Loss: 0.0001770391\n",
      "Epoch 149 of 50, Train Loss: 0.0001767824\n",
      "Epoch 150 of 50, Train Loss: 0.0001765526\n",
      "Epoch 151 of 50, Train Loss: 0.0001763445\n",
      "Epoch 152 of 50, Train Loss: 0.0001761461\n",
      "Epoch 153 of 50, Train Loss: 0.0001759567\n",
      "Epoch 154 of 50, Train Loss: 0.0001757377\n",
      "Epoch 155 of 50, Train Loss: 0.0001755099\n",
      "Epoch 156 of 50, Train Loss: 0.0001752474\n",
      "Epoch 157 of 50, Train Loss: 0.0001749734\n",
      "Epoch 158 of 50, Train Loss: 0.0001746915\n",
      "Epoch 159 of 50, Train Loss: 0.0001744099\n",
      "Epoch 160 of 50, Train Loss: 0.0001741110\n",
      "Epoch 161 of 50, Train Loss: 0.0001738279\n",
      "Epoch 162 of 50, Train Loss: 0.0001734650\n",
      "Epoch 163 of 50, Train Loss: 0.0001730596\n",
      "Epoch 164 of 50, Train Loss: 0.0001725769\n",
      "Epoch 165 of 50, Train Loss: 0.0001720842\n",
      "Epoch 166 of 50, Train Loss: 0.0001715336\n",
      "Epoch 167 of 50, Train Loss: 0.0001709000\n",
      "Epoch 168 of 50, Train Loss: 0.0001702074\n",
      "Epoch 169 of 50, Train Loss: 0.0001695439\n",
      "Epoch 170 of 50, Train Loss: 0.0001688721\n",
      "Epoch 171 of 50, Train Loss: 0.0001681829\n",
      "Epoch 172 of 50, Train Loss: 0.0001674606\n",
      "Epoch 173 of 50, Train Loss: 0.0001667082\n",
      "Epoch 174 of 50, Train Loss: 0.0001659151\n",
      "Epoch 175 of 50, Train Loss: 0.0001651724\n",
      "Epoch 176 of 50, Train Loss: 0.0001644566\n",
      "Epoch 177 of 50, Train Loss: 0.0001636601\n",
      "Epoch 178 of 50, Train Loss: 0.0001628426\n",
      "Epoch 179 of 50, Train Loss: 0.0001620096\n",
      "Epoch 180 of 50, Train Loss: 0.0001612085\n",
      "Epoch 181 of 50, Train Loss: 0.0001604364\n",
      "Epoch 182 of 50, Train Loss: 0.0001597252\n",
      "Epoch 183 of 50, Train Loss: 0.0001589805\n",
      "Epoch 184 of 50, Train Loss: 0.0001582271\n",
      "Epoch 185 of 50, Train Loss: 0.0001574834\n",
      "Epoch 186 of 50, Train Loss: 0.0001567836\n",
      "Epoch 187 of 50, Train Loss: 0.0001560962\n",
      "Epoch 188 of 50, Train Loss: 0.0001554414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189 of 50, Train Loss: 0.0001547688\n",
      "Epoch 190 of 50, Train Loss: 0.0001541002\n",
      "Epoch 191 of 50, Train Loss: 0.0001534637\n",
      "Epoch 192 of 50, Train Loss: 0.0001527567\n",
      "Epoch 193 of 50, Train Loss: 0.0001518481\n",
      "Epoch 194 of 50, Train Loss: 0.0001509607\n",
      "Epoch 195 of 50, Train Loss: 0.0001503781\n",
      "Epoch 196 of 50, Train Loss: 0.0001497978\n",
      "Epoch 197 of 50, Train Loss: 0.0001490250\n",
      "Epoch 198 of 50, Train Loss: 0.0001485606\n",
      "Epoch 199 of 50, Train Loss: 0.0001479874\n",
      "Epoch 200 of 50, Train Loss: 0.0001475447\n",
      "Epoch 201 of 50, Train Loss: 0.0001472019\n",
      "Epoch 202 of 50, Train Loss: 0.0001468336\n",
      "Epoch 203 of 50, Train Loss: 0.0001466629\n",
      "Epoch 204 of 50, Train Loss: 0.0001463931\n",
      "Epoch 205 of 50, Train Loss: 0.0001462633\n",
      "Epoch 206 of 50, Train Loss: 0.0001460790\n",
      "Epoch 207 of 50, Train Loss: 0.0001459054\n",
      "Epoch 208 of 50, Train Loss: 0.0001457667\n",
      "Epoch 209 of 50, Train Loss: 0.0001455429\n",
      "Epoch 210 of 50, Train Loss: 0.0001453102\n",
      "Epoch 211 of 50, Train Loss: 0.0001451368\n",
      "Epoch 212 of 50, Train Loss: 0.0001449169\n",
      "Epoch 213 of 50, Train Loss: 0.0001446023\n",
      "Epoch 214 of 50, Train Loss: 0.0001443631\n",
      "Epoch 215 of 50, Train Loss: 0.0001441964\n",
      "Epoch 216 of 50, Train Loss: 0.0001439656\n",
      "Epoch 217 of 50, Train Loss: 0.0001436797\n",
      "Epoch 218 of 50, Train Loss: 0.0001434324\n",
      "Epoch 219 of 50, Train Loss: 0.0001431374\n",
      "Epoch 220 of 50, Train Loss: 0.0001428659\n",
      "Epoch 221 of 50, Train Loss: 0.0001425990\n",
      "Epoch 222 of 50, Train Loss: 0.0001423149\n",
      "Epoch 223 of 50, Train Loss: 0.0001420673\n",
      "Epoch 224 of 50, Train Loss: 0.0001417940\n",
      "Epoch 225 of 50, Train Loss: 0.0001414765\n",
      "Epoch 226 of 50, Train Loss: 0.0001411877\n",
      "Epoch 227 of 50, Train Loss: 0.0001409043\n",
      "Epoch 228 of 50, Train Loss: 0.0001406172\n",
      "Epoch 229 of 50, Train Loss: 0.0001403539\n",
      "Epoch 230 of 50, Train Loss: 0.0001400841\n",
      "Epoch 231 of 50, Train Loss: 0.0001397986\n",
      "Epoch 232 of 50, Train Loss: 0.0001395102\n",
      "Epoch 233 of 50, Train Loss: 0.0001392096\n",
      "Epoch 234 of 50, Train Loss: 0.0001389270\n",
      "Epoch 235 of 50, Train Loss: 0.0001386580\n",
      "Epoch 236 of 50, Train Loss: 0.0001384019\n",
      "Epoch 237 of 50, Train Loss: 0.0001381618\n",
      "Epoch 238 of 50, Train Loss: 0.0001379571\n",
      "Epoch 239 of 50, Train Loss: 0.0001376995\n",
      "Epoch 240 of 50, Train Loss: 0.0001374423\n",
      "Epoch 241 of 50, Train Loss: 0.0001371886\n",
      "Epoch 242 of 50, Train Loss: 0.0001369635\n",
      "Epoch 243 of 50, Train Loss: 0.0001367666\n",
      "Epoch 244 of 50, Train Loss: 0.0001365806\n",
      "Epoch 245 of 50, Train Loss: 0.0001364155\n",
      "Epoch 246 of 50, Train Loss: 0.0001362289\n",
      "Epoch 247 of 50, Train Loss: 0.0001360137\n",
      "Epoch 248 of 50, Train Loss: 0.0001357768\n",
      "Epoch 249 of 50, Train Loss: 0.0001355267\n",
      "Epoch 250 of 50, Train Loss: 0.0001352673\n",
      "Epoch 251 of 50, Train Loss: 0.0001350029\n",
      "Epoch 252 of 50, Train Loss: 0.0001347278\n",
      "Epoch 253 of 50, Train Loss: 0.0001344556\n",
      "Epoch 254 of 50, Train Loss: 0.0001341912\n",
      "Epoch 255 of 50, Train Loss: 0.0001339559\n",
      "Epoch 256 of 50, Train Loss: 0.0001336969\n",
      "Epoch 257 of 50, Train Loss: 0.0001334671\n",
      "Epoch 258 of 50, Train Loss: 0.0001332777\n",
      "Epoch 259 of 50, Train Loss: 0.0001330935\n",
      "Epoch 260 of 50, Train Loss: 0.0001328955\n",
      "Epoch 261 of 50, Train Loss: 0.0001326787\n",
      "Epoch 262 of 50, Train Loss: 0.0001324475\n",
      "Epoch 263 of 50, Train Loss: 0.0001322180\n",
      "Epoch 264 of 50, Train Loss: 0.0001319695\n",
      "Epoch 265 of 50, Train Loss: 0.0001317196\n",
      "Epoch 266 of 50, Train Loss: 0.0001314587\n",
      "Epoch 267 of 50, Train Loss: 0.0001312204\n",
      "Epoch 268 of 50, Train Loss: 0.0001310110\n",
      "Epoch 269 of 50, Train Loss: 0.0001308372\n",
      "Epoch 270 of 50, Train Loss: 0.0001306763\n",
      "Epoch 271 of 50, Train Loss: 0.0001305415\n",
      "Epoch 272 of 50, Train Loss: 0.0001303714\n",
      "Epoch 273 of 50, Train Loss: 0.0001301955\n",
      "Epoch 274 of 50, Train Loss: 0.0001300213\n",
      "Epoch 275 of 50, Train Loss: 0.0001298500\n",
      "Epoch 276 of 50, Train Loss: 0.0001296766\n",
      "Epoch 277 of 50, Train Loss: 0.0001295244\n",
      "Epoch 278 of 50, Train Loss: 0.0001293810\n",
      "Epoch 279 of 50, Train Loss: 0.0001292265\n",
      "Epoch 280 of 50, Train Loss: 0.0001290782\n",
      "Epoch 281 of 50, Train Loss: 0.0001289510\n",
      "Epoch 282 of 50, Train Loss: 0.0001288115\n",
      "Epoch 283 of 50, Train Loss: 0.0001286110\n",
      "Epoch 284 of 50, Train Loss: 0.0001283667\n",
      "Epoch 285 of 50, Train Loss: 0.0001281197\n",
      "Epoch 286 of 50, Train Loss: 0.0001278780\n",
      "Epoch 287 of 50, Train Loss: 0.0001276549\n",
      "Epoch 288 of 50, Train Loss: 0.0001274243\n",
      "Epoch 289 of 50, Train Loss: 0.0001271932\n",
      "Epoch 290 of 50, Train Loss: 0.0001269400\n",
      "Epoch 291 of 50, Train Loss: 0.0001266879\n",
      "Epoch 292 of 50, Train Loss: 0.0001264280\n",
      "Epoch 293 of 50, Train Loss: 0.0001261809\n",
      "Epoch 294 of 50, Train Loss: 0.0001259587\n",
      "Epoch 295 of 50, Train Loss: 0.0001257388\n",
      "Epoch 296 of 50, Train Loss: 0.0001255432\n",
      "Epoch 297 of 50, Train Loss: 0.0001253850\n",
      "Epoch 298 of 50, Train Loss: 0.0001252666\n",
      "Epoch 299 of 50, Train Loss: 0.0001251655\n",
      "Epoch 300 of 50, Train Loss: 0.0001250453\n",
      "Epoch 301 of 50, Train Loss: 0.0001249223\n",
      "Epoch 302 of 50, Train Loss: 0.0001248080\n",
      "Epoch 303 of 50, Train Loss: 0.0001247088\n",
      "Epoch 304 of 50, Train Loss: 0.0001246213\n",
      "Epoch 305 of 50, Train Loss: 0.0001244990\n",
      "Epoch 306 of 50, Train Loss: 0.0001243317\n",
      "Epoch 307 of 50, Train Loss: 0.0001241519\n",
      "Epoch 308 of 50, Train Loss: 0.0001239350\n",
      "Epoch 309 of 50, Train Loss: 0.0001237158\n",
      "Epoch 310 of 50, Train Loss: 0.0001235103\n",
      "Epoch 311 of 50, Train Loss: 0.0001233332\n",
      "Epoch 312 of 50, Train Loss: 0.0001231807\n",
      "Epoch 313 of 50, Train Loss: 0.0001230298\n",
      "Epoch 314 of 50, Train Loss: 0.0001228920\n",
      "Epoch 315 of 50, Train Loss: 0.0001227778\n",
      "Epoch 316 of 50, Train Loss: 0.0001226731\n",
      "Epoch 317 of 50, Train Loss: 0.0001225634\n",
      "Epoch 318 of 50, Train Loss: 0.0001224614\n",
      "Epoch 319 of 50, Train Loss: 0.0001223026\n",
      "Epoch 320 of 50, Train Loss: 0.0001221300\n",
      "Epoch 321 of 50, Train Loss: 0.0001219008\n",
      "Epoch 322 of 50, Train Loss: 0.0001216353\n",
      "Epoch 323 of 50, Train Loss: 0.0001213525\n",
      "Epoch 324 of 50, Train Loss: 0.0001210583\n",
      "Epoch 325 of 50, Train Loss: 0.0001207746\n",
      "Epoch 326 of 50, Train Loss: 0.0001204774\n",
      "Epoch 327 of 50, Train Loss: 0.0001202101\n",
      "Epoch 328 of 50, Train Loss: 0.0001199825\n",
      "Epoch 329 of 50, Train Loss: 0.0001197939\n",
      "Epoch 330 of 50, Train Loss: 0.0001196276\n",
      "Epoch 331 of 50, Train Loss: 0.0001194377\n",
      "Epoch 332 of 50, Train Loss: 0.0001192371\n",
      "Epoch 333 of 50, Train Loss: 0.0001190533\n",
      "Epoch 334 of 50, Train Loss: 0.0001188957\n",
      "Epoch 335 of 50, Train Loss: 0.0001187422\n",
      "Epoch 336 of 50, Train Loss: 0.0001185960\n",
      "Epoch 337 of 50, Train Loss: 0.0001184435\n",
      "Epoch 338 of 50, Train Loss: 0.0001182749\n",
      "Epoch 339 of 50, Train Loss: 0.0001181021\n",
      "Epoch 340 of 50, Train Loss: 0.0001179177\n",
      "Epoch 341 of 50, Train Loss: 0.0001177035\n",
      "Epoch 342 of 50, Train Loss: 0.0001174855\n",
      "Epoch 343 of 50, Train Loss: 0.0001172828\n",
      "Epoch 344 of 50, Train Loss: 0.0001170870\n",
      "Epoch 345 of 50, Train Loss: 0.0001168703\n",
      "Epoch 346 of 50, Train Loss: 0.0001166489\n",
      "Epoch 347 of 50, Train Loss: 0.0001164192\n",
      "Epoch 348 of 50, Train Loss: 0.0001161961\n",
      "Epoch 349 of 50, Train Loss: 0.0001159601\n",
      "Epoch 350 of 50, Train Loss: 0.0001157216\n",
      "Epoch 351 of 50, Train Loss: 0.0001154545\n",
      "Epoch 352 of 50, Train Loss: 0.0001152263\n",
      "Epoch 353 of 50, Train Loss: 0.0001150561\n",
      "Epoch 354 of 50, Train Loss: 0.0001149444\n",
      "Epoch 355 of 50, Train Loss: 0.0001148666\n",
      "Epoch 356 of 50, Train Loss: 0.0001147758\n",
      "Epoch 357 of 50, Train Loss: 0.0001146657\n",
      "Epoch 358 of 50, Train Loss: 0.0001145181\n",
      "Epoch 359 of 50, Train Loss: 0.0001143800\n",
      "Epoch 360 of 50, Train Loss: 0.0001142533\n",
      "Epoch 361 of 50, Train Loss: 0.0001141335\n",
      "Epoch 362 of 50, Train Loss: 0.0001140248\n",
      "Epoch 363 of 50, Train Loss: 0.0001138947\n",
      "Epoch 364 of 50, Train Loss: 0.0001137624\n",
      "Epoch 365 of 50, Train Loss: 0.0001136019\n",
      "Epoch 366 of 50, Train Loss: 0.0001134533\n",
      "Epoch 367 of 50, Train Loss: 0.0001133175\n",
      "Epoch 368 of 50, Train Loss: 0.0001131979\n",
      "Epoch 369 of 50, Train Loss: 0.0001130969\n",
      "Epoch 370 of 50, Train Loss: 0.0001130075\n",
      "Epoch 371 of 50, Train Loss: 0.0001129159\n",
      "Epoch 372 of 50, Train Loss: 0.0001128111\n",
      "Epoch 373 of 50, Train Loss: 0.0001126994\n",
      "Epoch 374 of 50, Train Loss: 0.0001125839\n",
      "Epoch 375 of 50, Train Loss: 0.0001124475\n",
      "Epoch 376 of 50, Train Loss: 0.0001123161\n",
      "Epoch 377 of 50, Train Loss: 0.0001121897\n",
      "Epoch 378 of 50, Train Loss: 0.0001120458\n",
      "Epoch 379 of 50, Train Loss: 0.0001119033\n",
      "Epoch 380 of 50, Train Loss: 0.0001117412\n",
      "Epoch 381 of 50, Train Loss: 0.0001115816\n",
      "Epoch 382 of 50, Train Loss: 0.0001114215\n",
      "Epoch 383 of 50, Train Loss: 0.0001112717\n",
      "Epoch 384 of 50, Train Loss: 0.0001111387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 385 of 50, Train Loss: 0.0001110329\n",
      "Epoch 386 of 50, Train Loss: 0.0001109571\n",
      "Epoch 387 of 50, Train Loss: 0.0001109113\n",
      "Epoch 388 of 50, Train Loss: 0.0001108897\n",
      "Epoch 389 of 50, Train Loss: 0.0001108875\n",
      "Epoch 390 of 50, Train Loss: 0.0001108984\n",
      "Epoch 391 of 50, Train Loss: 0.0001109094\n",
      "Epoch 392 of 50, Train Loss: 0.0001109095\n",
      "Epoch 393 of 50, Train Loss: 0.0001108845\n",
      "Epoch 394 of 50, Train Loss: 0.0001108478\n",
      "Epoch 395 of 50, Train Loss: 0.0001107982\n",
      "Epoch 396 of 50, Train Loss: 0.0001107283\n",
      "Epoch 397 of 50, Train Loss: 0.0001106356\n",
      "Epoch 398 of 50, Train Loss: 0.0001105383\n",
      "Epoch 399 of 50, Train Loss: 0.0001104464\n",
      "Epoch 400 of 50, Train Loss: 0.0001103618\n",
      "Epoch 401 of 50, Train Loss: 0.0001102863\n",
      "Epoch 402 of 50, Train Loss: 0.0001102255\n",
      "Epoch 403 of 50, Train Loss: 0.0001101823\n",
      "Epoch 404 of 50, Train Loss: 0.0001101144\n",
      "Epoch 405 of 50, Train Loss: 0.0001099731\n",
      "Epoch 406 of 50, Train Loss: 0.0001097890\n",
      "Epoch 407 of 50, Train Loss: 0.0001096001\n",
      "Epoch 408 of 50, Train Loss: 0.0001093890\n",
      "Epoch 409 of 50, Train Loss: 0.0001092003\n",
      "Epoch 410 of 50, Train Loss: 0.0001090355\n",
      "Epoch 411 of 50, Train Loss: 0.0001088803\n",
      "Epoch 412 of 50, Train Loss: 0.0001087398\n",
      "Epoch 413 of 50, Train Loss: 0.0001085811\n",
      "Epoch 414 of 50, Train Loss: 0.0001084142\n",
      "Epoch 415 of 50, Train Loss: 0.0001082158\n",
      "Epoch 416 of 50, Train Loss: 0.0001080221\n",
      "Epoch 417 of 50, Train Loss: 0.0001077343\n",
      "Epoch 418 of 50, Train Loss: 0.0001074345\n",
      "Epoch 419 of 50, Train Loss: 0.0001071379\n",
      "Epoch 420 of 50, Train Loss: 0.0001068816\n",
      "Epoch 421 of 50, Train Loss: 0.0001066494\n",
      "Epoch 422 of 50, Train Loss: 0.0001064191\n",
      "Epoch 423 of 50, Train Loss: 0.0001061467\n",
      "Epoch 424 of 50, Train Loss: 0.0001058541\n",
      "Epoch 425 of 50, Train Loss: 0.0001055543\n",
      "Epoch 426 of 50, Train Loss: 0.0001053067\n",
      "Epoch 427 of 50, Train Loss: 0.0001051176\n",
      "Epoch 428 of 50, Train Loss: 0.0001049523\n",
      "Epoch 429 of 50, Train Loss: 0.0001047952\n",
      "Epoch 430 of 50, Train Loss: 0.0001045745\n",
      "Epoch 431 of 50, Train Loss: 0.0001043529\n",
      "Epoch 432 of 50, Train Loss: 0.0001041072\n",
      "Epoch 433 of 50, Train Loss: 0.0001038843\n",
      "Epoch 434 of 50, Train Loss: 0.0001036786\n",
      "Epoch 435 of 50, Train Loss: 0.0001034886\n",
      "Epoch 436 of 50, Train Loss: 0.0001032942\n",
      "Epoch 437 of 50, Train Loss: 0.0001030899\n",
      "Epoch 438 of 50, Train Loss: 0.0001029031\n",
      "Epoch 439 of 50, Train Loss: 0.0001027337\n",
      "Epoch 440 of 50, Train Loss: 0.0001025982\n",
      "Epoch 441 of 50, Train Loss: 0.0001024778\n",
      "Epoch 442 of 50, Train Loss: 0.0001023785\n",
      "Epoch 443 of 50, Train Loss: 0.0001022995\n",
      "Epoch 444 of 50, Train Loss: 0.0001021920\n",
      "Epoch 445 of 50, Train Loss: 0.0001020453\n",
      "Epoch 446 of 50, Train Loss: 0.0001018562\n",
      "Epoch 447 of 50, Train Loss: 0.0001016308\n",
      "Epoch 448 of 50, Train Loss: 0.0001013887\n",
      "Epoch 449 of 50, Train Loss: 0.0001011506\n",
      "Epoch 450 of 50, Train Loss: 0.0001009375\n",
      "Epoch 451 of 50, Train Loss: 0.0001007604\n",
      "Epoch 452 of 50, Train Loss: 0.0001006228\n",
      "Epoch 453 of 50, Train Loss: 0.0001005233\n",
      "Epoch 454 of 50, Train Loss: 0.0001004528\n",
      "Epoch 455 of 50, Train Loss: 0.0001003788\n",
      "Epoch 456 of 50, Train Loss: 0.0001003087\n",
      "Epoch 457 of 50, Train Loss: 0.0001002410\n",
      "Epoch 458 of 50, Train Loss: 0.0001001769\n",
      "Epoch 459 of 50, Train Loss: 0.0001001065\n",
      "Epoch 460 of 50, Train Loss: 0.0001000377\n",
      "Epoch 461 of 50, Train Loss: 0.0000999737\n",
      "Epoch 462 of 50, Train Loss: 0.0000999065\n",
      "Epoch 463 of 50, Train Loss: 0.0000998424\n",
      "Epoch 464 of 50, Train Loss: 0.0000997641\n",
      "Epoch 465 of 50, Train Loss: 0.0000996917\n",
      "Epoch 466 of 50, Train Loss: 0.0000996225\n",
      "Epoch 467 of 50, Train Loss: 0.0000995623\n",
      "Epoch 468 of 50, Train Loss: 0.0000995118\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-276-22ee8d3ed0a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_prices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_prices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-254-f84ff21ef524>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1690\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = Autoencoder(feature_len=prices_numpy.shape[1])\n",
    "print(net)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "torch_prices = torch.from_numpy(prices_scaled).float()\n",
    "# torch_prices = torch.from_numpy(prices_numpy).float()\n",
    "\n",
    "\n",
    "train_loss = []\n",
    "for epoch in range(500):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    torch_prices = torch_prices.to(device)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = net(torch_prices)\n",
    "    loss = criterion(outputs, torch_prices)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "\n",
    "    loss = running_loss / len(trainloader)\n",
    "    train_loss.append(loss)\n",
    "    print('Epoch {} of {}, Train Loss: {:.10f}'.format(\n",
    "        epoch+1, NUM_EPOCHS, loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(torch_prices)\n",
    "\n",
    "outputs = outputs.cpu().data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "inversed = min_max_scaler.inverse_transform(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([234, 2786])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3534, 0.3747, 0.3665, 0.3797, 0.3779, 0.3623, 0.3892, 0.3782, 0.3782,\n",
       "        0.4070, 0.3645, 0.3507, 0.3673, 0.3668, 0.3355, 0.3303, 0.3934, 0.3486,\n",
       "        0.3631, 0.3139, 0.3936, 0.4059, 0.4070, 0.3661, 0.3387, 0.3824, 0.3918,\n",
       "        0.3889, 0.3713, 0.3586, 0.3645, 0.3887, 0.3768, 0.3422, 0.2887, 0.2817,\n",
       "        0.3447, 0.2730, 0.3376, 0.4431, 0.3096, 0.4447, 0.2773, 0.3089, 0.1565,\n",
       "        0.4456, 0.2224, 0.2011, 0.5372, 0.2978, 0.5298, 0.2737, 0.5558, 0.3000,\n",
       "        0.3138, 0.5934, 0.4291, 0.5350, 0.2579, 0.4301, 0.3710, 0.2072, 0.3994,\n",
       "        0.2906, 0.5540, 0.3857, 0.4782, 0.4796, 0.3140, 0.4201, 0.2706, 0.3480,\n",
       "        0.4834, 0.3320, 0.3075, 0.4098, 0.3978, 0.4068, 0.4793, 0.4029, 0.5020,\n",
       "        0.2871, 0.2755, 0.3771, 0.3669, 0.3454, 0.4146, 0.4614, 0.3437, 0.2843,\n",
       "        0.2807, 0.3765, 0.4061, 0.5205, 0.3171, 0.4587, 0.3612, 0.3860, 0.4552,\n",
       "        0.4577, 0.3143, 0.3621, 0.3817, 0.3937, 0.4504, 0.3881, 0.4705, 0.4339,\n",
       "        0.3294, 0.3027, 0.1833, 0.4323, 0.4197, 0.4339, 0.3242, 0.3739, 0.3709,\n",
       "        0.3827, 0.3771, 0.2838, 0.4148, 0.2977, 0.4421, 0.4109, 0.3408, 0.3783,\n",
       "        0.3981, 0.3086, 0.3807, 0.3170, 0.4201, 0.3383, 0.3969, 0.4597, 0.3523,\n",
       "        0.3768, 0.3692, 0.4153, 0.3560, 0.3674, 0.3370, 0.3880, 0.3484, 0.4163,\n",
       "        0.3643, 0.3431, 0.4127, 0.3873, 0.4090, 0.3722, 0.4133, 0.3985, 0.3655,\n",
       "        0.3868, 0.3616, 0.3702, 0.3757, 0.3412, 0.3732, 0.3523, 0.3447, 0.3985,\n",
       "        0.3835, 0.3533, 0.3752, 0.3983, 0.3494, 0.3831, 0.3975, 0.3001, 0.3601,\n",
       "        0.3186, 0.3991, 0.3407, 0.3566, 0.4337, 0.3704, 0.3873, 0.3613, 0.3687,\n",
       "        0.2782, 0.3701, 0.2957, 0.3704, 0.4091, 0.4358, 0.3677, 0.3714, 0.4017,\n",
       "        0.3793, 0.4396, 0.3613, 0.4320, 0.4028, 0.3847, 0.3901, 0.3452, 0.3445,\n",
       "        0.3936, 0.3713, 0.3405, 0.3848, 0.3577, 0.4103, 0.3896, 0.3141, 0.3407,\n",
       "        0.2917, 0.4079, 0.3376, 0.4148, 0.4444, 0.3568, 0.4397, 0.3424, 0.4906,\n",
       "        0.4190, 0.3705, 0.3224, 0.4266, 0.4354, 0.3726, 0.3442, 0.3907, 0.3704,\n",
       "        0.4069, 0.4350, 0.3616, 0.3840, 0.3294, 0.3940, 0.3820, 0.3843, 0.4294])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.58477941, 0.34796427, 0.40346791, 0.5175801 , 0.32241814,\n",
       "       0.44566968, 0.45859635, 0.44372618, 0.48242622, 0.42880644,\n",
       "       0.11564978, 0.41803757, 0.57706403, 0.28348247, 0.29014816,\n",
       "       0.32847683, 0.32561476, 0.2415826 , 0.40346791, 0.40346791,\n",
       "       0.40346791, 0.38588383, 0.42120488, 0.36829976, 0.20664947,\n",
       "       0.5420921 , 0.3663815 , 0.34680789, 0.50059921, 0.49618392,\n",
       "       0.65178581, 0.2137233 , 0.52550459, 0.1896227 , 0.2932108 ,\n",
       "       0.57830407, 0.38557534, 0.27711134, 0.34573906, 0.30445079,\n",
       "       0.40346791, 0.44509551, 0.32187769, 0.48845772, 0.34227533,\n",
       "       0.42449628, 0.36184018, 0.27598325, 0.31281224, 0.37974985,\n",
       "       0.54745057, 0.        , 0.43140972, 0.84449578, 0.26748427,\n",
       "       0.45203359, 0.5220583 , 0.60520179, 0.32187769, 0.38222048,\n",
       "       0.40346791, 0.40346791, 0.33905467, 0.46998157, 0.40346791,\n",
       "       0.29611238, 0.35814007, 0.33393089, 0.3314765 , 0.40346791,\n",
       "       0.50296817, 0.37974985, 0.37947068, 0.37918514, 0.25601577,\n",
       "       0.37697761, 0.34979021, 0.43103213, 0.18589415, 0.64702061,\n",
       "       0.32187781, 0.40346791, 0.46012769, 0.32077508, 0.43219698,\n",
       "       0.48845759, 0.53945156, 0.27598323, 0.4306646 , 0.2692735 ,\n",
       "       0.54711262, 0.29611251, 0.37513784, 0.43219698, 0.51678748,\n",
       "       0.48398461, 0.68748421, 0.24482035, 0.57549537, 0.40346791,\n",
       "       0.49412371, 0.3166698 , 0.42613196, 0.33622319, 0.47300506,\n",
       "       0.29139346, 0.40346791, 0.47462206, 0.47222374, 0.58083781,\n",
       "       0.64823844, 0.43989206, 0.6718566 , 0.13466312, 0.40346791,\n",
       "       0.18492278, 0.30148016, 0.81141878, 0.4571456 , 0.54293837,\n",
       "       0.37083187, 0.27080088, 0.43894186, 0.56037215, 0.48441046,\n",
       "       0.40346791, 0.48132124, 0.41846608, 0.56724383, 0.41725002,\n",
       "       0.33501984, 0.4317978 , 0.57111895, 0.3260088 , 0.64501786,\n",
       "       0.41546645, 0.00983105, 0.30000203, 0.37232665, 0.34021977,\n",
       "       0.25660553, 0.36829976, 0.36768277, 0.31240753, 0.36534147,\n",
       "       0.42289412, 1.        , 0.25458069, 0.45165103, 0.35639669,\n",
       "       0.38740686, 0.32252535, 0.30232296, 0.31478301, 0.40346791,\n",
       "       0.40346791, 0.47764072, 0.49293095, 0.24920072, 0.47764072,\n",
       "       0.38557534, 0.31321337, 0.4223545 , 0.42218123, 0.32929487,\n",
       "       0.67286964, 0.26748421, 0.49452828, 0.33373278, 0.36736609,\n",
       "       0.38509176, 0.47764072, 0.31400508, 0.36604126, 0.38440457,\n",
       "       0.46119698, 0.42218123, 0.44055431, 0.34883168, 0.34732771,\n",
       "       0.40346791, 0.42271108, 0.42253101, 0.38458131, 0.46065722,\n",
       "       0.49618392, 0.36799395, 0.36736609, 0.4402202 , 0.34931519,\n",
       "       0.42201111, 0.40346791, 0.38509176, 0.3849247 , 0.38475458,\n",
       "       0.61122069, 0.23205994, 0.51574785, 0.43894186, 0.43833568,\n",
       "       0.45489023, 0.45362575, 0.28924157, 0.54175642, 0.35490237,\n",
       "       0.18788412, 0.44055431, 0.34883168, 0.40346791, 0.40346791,\n",
       "       0.42218123, 0.45909752, 0.40346791, 0.40346791, 0.45762063,\n",
       "       0.38588383, 0.40346791, 0.38573093, 0.54660864, 0.25299417,\n",
       "       0.385417  , 0.42167998, 0.45762063, 0.40346791, 0.38588383,\n",
       "       0.43894186, 0.47320324, 0.50461265, 0.35528478, 0.37056867,\n",
       "       0.28643273, 0.43894186, 0.50807081, 0.32055094])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_scaled[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
